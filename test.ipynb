{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting words tokenized ...\n",
      "Time taken 1.3822641000006115 sec\n",
      "Getting words stemmed ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\qq522\\OneDrive\\桌面\\Project broser shield T1\\test.ipynb Cell 1\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qq522/OneDrive/%E6%A1%8C%E9%9D%A2/Project%20broser%20shield%20T1/test.ipynb#W0sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mGetting words stemmed ...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qq522/OneDrive/%E6%A1%8C%E9%9D%A2/Project%20broser%20shield%20T1/test.ipynb#W0sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m t0\u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/qq522/OneDrive/%E6%A1%8C%E9%9D%A2/Project%20broser%20shield%20T1/test.ipynb#W0sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mtext_stemmed\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mtext_tokenized\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mmap(\u001b[39mlambda\u001b[39;49;00m l: [stemmer\u001b[39m.\u001b[39;49mstem(word) \u001b[39mfor\u001b[39;49;00m word \u001b[39min\u001b[39;49;00m l])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qq522/OneDrive/%E6%A1%8C%E9%9D%A2/Project%20broser%20shield%20T1/test.ipynb#W0sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m t1\u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter() \u001b[39m-\u001b[39m t0\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qq522/OneDrive/%E6%A1%8C%E9%9D%A2/Project%20broser%20shield%20T1/test.ipynb#W0sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTime taken\u001b[39m\u001b[39m'\u001b[39m,t1 ,\u001b[39m'\u001b[39m\u001b[39msec\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\qq522\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:4540\u001b[0m, in \u001b[0;36mSeries.map\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   4460\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap\u001b[39m(\n\u001b[0;32m   4461\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4462\u001b[0m     arg: Callable \u001b[39m|\u001b[39m Mapping \u001b[39m|\u001b[39m Series,\n\u001b[0;32m   4463\u001b[0m     na_action: Literal[\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   4464\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Series:\n\u001b[0;32m   4465\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4466\u001b[0m \u001b[39m    Map values of Series according to an input mapping or function.\u001b[39;00m\n\u001b[0;32m   4467\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4538\u001b[0m \u001b[39m    dtype: object\u001b[39;00m\n\u001b[0;32m   4539\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4540\u001b[0m     new_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_values(arg, na_action\u001b[39m=\u001b[39;49mna_action)\n\u001b[0;32m   4541\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor(new_values, index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\u001b[39m.\u001b[39m__finalize__(\n\u001b[0;32m   4542\u001b[0m         \u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmap\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   4543\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\qq522\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[39mreturn\u001b[39;00m arr\u001b[39m.\u001b[39mmap(mapper, na_action\u001b[39m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[39mreturn\u001b[39;00m algorithms\u001b[39m.\u001b[39;49mmap_array(arr, mapper, na_action\u001b[39m=\u001b[39;49mna_action, convert\u001b[39m=\u001b[39;49mconvert)\n",
      "File \u001b[1;32mc:\\Users\\qq522\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[39mif\u001b[39;00m na_action \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39;49mmap_infer(values, mapper, convert\u001b[39m=\u001b[39;49mconvert)\n\u001b[0;32m   1815\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[39mreturn\u001b[39;00m lib\u001b[39m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[39m=\u001b[39misna(values)\u001b[39m.\u001b[39mview(np\u001b[39m.\u001b[39muint8), convert\u001b[39m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2920\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\qq522\\OneDrive\\桌面\\Project broser shield T1\\test.ipynb Cell 1\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qq522/OneDrive/%E6%A1%8C%E9%9D%A2/Project%20broser%20shield%20T1/test.ipynb#W0sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mGetting words stemmed ...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qq522/OneDrive/%E6%A1%8C%E9%9D%A2/Project%20broser%20shield%20T1/test.ipynb#W0sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m t0\u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/qq522/OneDrive/%E6%A1%8C%E9%9D%A2/Project%20broser%20shield%20T1/test.ipynb#W0sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mtext_stemmed\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mtext_tokenized\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mmap(\u001b[39mlambda\u001b[39;00m l: [stemmer\u001b[39m.\u001b[39;49mstem(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m l])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qq522/OneDrive/%E6%A1%8C%E9%9D%A2/Project%20broser%20shield%20T1/test.ipynb#W0sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m t1\u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter() \u001b[39m-\u001b[39m t0\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/qq522/OneDrive/%E6%A1%8C%E9%9D%A2/Project%20broser%20shield%20T1/test.ipynb#W0sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTime taken\u001b[39m\u001b[39m'\u001b[39m,t1 ,\u001b[39m'\u001b[39m\u001b[39msec\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\qq522\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\stem\\snowball.py:1578\u001b[0m, in \u001b[0;36mEnglishStemmer.stem\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m   1576\u001b[0m \u001b[39m# STEP 2\u001b[39;00m\n\u001b[0;32m   1577\u001b[0m \u001b[39mfor\u001b[39;00m suffix \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__step2_suffixes:\n\u001b[1;32m-> 1578\u001b[0m     \u001b[39mif\u001b[39;00m word\u001b[39m.\u001b[39mendswith(suffix):\n\u001b[0;32m   1579\u001b[0m         \u001b[39mif\u001b[39;00m r1\u001b[39m.\u001b[39mendswith(suffix):\n\u001b[0;32m   1580\u001b[0m             \u001b[39mif\u001b[39;00m suffix \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtional\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# importing some useful libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns  \n",
    "import time \n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.tokenize import RegexpTokenizer  \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import pickle \n",
    "\n",
    "# Loading the dataset\n",
    "df= pd.read_csv(\"phishing_site_urls.csv\")\n",
    "tokenizer = RegexpTokenizer(r'[A-Za-z]+')\n",
    "tokenizer.tokenize(df.URL[0]) # this will fetch all the words from the first URL\n",
    "\n",
    "# Tokenizing all the rows \n",
    "print('Getting words tokenized ...')\n",
    "t0= time.perf_counter()\n",
    "df['text_tokenized'] = df.URL.map(lambda t: tokenizer.tokenize(t))\n",
    "t1 = time.perf_counter() - t0\n",
    "print('Time taken',t1 ,'sec')\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\") # choose a language\n",
    "# Getting all the stemmed words\n",
    "print('Getting words stemmed ...')\n",
    "t0= time.perf_counter()\n",
    "df['text_stemmed'] = df['text_tokenized'].map(lambda l: [stemmer.stem(word) for word in l])\n",
    "t1= time.perf_counter() - t0\n",
    "print('Time taken',t1 ,'sec')\n",
    "\n",
    "# Joining all the stemmmed words.\n",
    "print('Get joiningwords ...')\n",
    "t0= time.perf_counter()\n",
    "df['text_sent'] = df['text_stemmed'].map(lambda l: ' '.join(l))\n",
    "t1= time.perf_counter() - t0\n",
    "print('Time taken',t1 ,'sec')\n",
    "\n",
    "bad_sites = df[df.Label == 'bad']\n",
    "good_sites = df[df.Label == 'good']\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "feature = cv.fit_transform(df.text_sent) #transform all text which we tokenize and stemed\n",
    "\n",
    "feature[:5].toarray() # convert sparse matrix into array to print transformed features\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(feature, df.Label)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(trainX,trainY)\n",
    "Scores_ml = {}\n",
    "Scores_ml['Logistic Regression'] = np.round(lr.score(testX,testY),2)\n",
    "# creating confusing matrix\n",
    "print('Training Accuracy :',lr.score(trainX,trainY))\n",
    "print('Testing Accuracy :',lr.score(testX,testY))\n",
    "con_mat = pd.DataFrame(confusion_matrix(lr.predict(testX), testY),\n",
    "            columns = ['Predicted:Bad', 'Predicted:Good'],\n",
    "            index = ['Actual:Bad', 'Actual:Good'])\n",
    "\n",
    "\n",
    "print('\\nCLASSIFICATION REPORT\\n')\n",
    "print(classification_report(lr.predict(testX), testY,\n",
    "                            target_names =['Bad','Good']))\n",
    "\n",
    "print('\\nCONFUSION MATRIX')\n",
    "plt.figure(figsize= (6,4))\n",
    "sns.heatmap(con_mat, annot = True,fmt='d',cmap=\"YlGnBu\")\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(trainX,trainY)\n",
    "mnb.score(testX,testY)\n",
    "Scores_ml['MultinomialNB'] = np.round(mnb.score(testX,testY),2)\n",
    "\n",
    "print('Training Accuracy :',mnb.score(trainX,trainY))\n",
    "print('Testing Accuracy :',mnb.score(testX,testY))\n",
    "con_mat = pd.DataFrame(confusion_matrix(mnb.predict(testX), testY),\n",
    "            columns = ['Predicted:Bad', 'Predicted:Good'],\n",
    "            index = ['Actual:Bad', 'Actual:Good'])\n",
    "\n",
    "\n",
    "print('\\nCLASSIFICATION REPORT\\n')\n",
    "print(classification_report(mnb.predict(testX), testY,\n",
    "                            target_names =['Bad','Good']))\n",
    "\n",
    "print('\\nCONFUSION MATRIX')\n",
    "plt.figure(figsize= (6,4))\n",
    "sns.heatmap(con_mat, annot = True,fmt='d',cmap=\"YlGnBu\")\n",
    "pipeline_ls = make_pipeline(CountVectorizer(tokenizer = RegexpTokenizer(r'[A-Za-z]+').tokenize,stop_words='english'), LogisticRegression())\n",
    "\n",
    "trainX, testX, trainY, testY = train_test_split(df.URL, df.Label)\n",
    "pipeline_ls.fit(trainX,trainY)\n",
    "print('Training Accuracy :',pipeline_ls.score(trainX,trainY))\n",
    "print('Testing Accuracy :',pipeline_ls.score(testX,testY))\n",
    "con_mat = pd.DataFrame(confusion_matrix(pipeline_ls.predict(testX), testY),\n",
    "            columns = ['Predicted:Bad', 'Predicted:Good'],\n",
    "            index = ['Actual:Bad', 'Actual:Good'])\n",
    "\n",
    "\n",
    "print('\\nCLASSIFICATION REPORT\\n')\n",
    "print(classification_report(pipeline_ls.predict(testX), testY,\n",
    "                            target_names =['Bad','Good']))\n",
    "\n",
    "print('\\nCONFUSION MATRIX')\n",
    "plt.figure(figsize= (6,4))\n",
    "sns.heatmap(con_mat, annot = True,fmt='d',cmap=\"YlGnBu\")\n",
    "import pickle\n",
    "\n",
    "# Assuming `pipeline_ls` is your chosen and fully trained model\n",
    "pickle.dump(pipeline_ls, open('phishing.pkl', 'wb'))\n",
    "print(\"Model saved successfully.\")\n",
    "loaded_model = pickle.load(open('phishing.pkl', 'rb'))\n",
    "result = loaded_model.score(testX, testY)\n",
    "print(\"Loaded model performance: \", result)\n",
    "print(\"end of training and saving model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
